<!DOCTYPE html>
<!--[if lt IE 7 ]><html class="ie ie6" lang="en"> <![endif]-->
<!--[if IE 7 ]><html class="ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="ie ie8" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<html lang="en">
<!--<![endif]-->

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <meta name="keywords" content="">

    <title>CS224n | Group Study</title>

    <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon">

    <link rel="stylesheet" type="text/css" href="fonts/font-awesome-4.3.0/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="css/stroke.css">
    <link rel="stylesheet" type="text/css" href="css/bootstrap.css">
    <link rel="stylesheet" type="text/css" href="css/animate.css">
    <link rel="stylesheet" type="text/css" href="css/prettyPhoto.css">
    <link rel="stylesheet" type="text/css" href="css/style.css">

    <link rel="stylesheet" type="text/css" href="js/syntax-highlighter/styles/shCore.css" media="all">
    <link rel="stylesheet" type="text/css" href="js/syntax-highlighter/styles/shThemeRDark.css" media="all">

    <!-- CUSTOM -->
    <link rel="stylesheet" type="text/css" href="css/custom.css">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- <style>
        /* webkit printing magic: print all background colors */
        html {
            -webkit-print-color-adjust: exact;
        }
        * {
            box-sizing: border-box;
            -webkit-print-color-adjust: exact;
        }
        
        html,
        body {
            margin: 0;
            padding: 0;
        }
        @media only screen {
            body {
                margin: 2em auto;
                max-width: 900px;
                color: rgb(55, 53, 47);
            }
        }
        
        body {
            line-height: 1.5;
            white-space: pre-wrap;
        }
        
        a,
        a.visited {
            color: inherit;
            text-decoration: underline;
        }
        
        .pdf-relative-link-path {
            font-size: 80%;
            color: #444;
        }
        
        h1,
        h2,
        h3 {
            letter-spacing: -0.01em;
            line-height: 1.2;
            font-weight: 600;
            margin-bottom: 0;
        }
        
        .page-title {
            font-size: 2.5rem;
            font-weight: 700;
            margin-top: 0;
            margin-bottom: 0.75em;
        }
        
        h1 {
            font-size: 1.875rem;
            margin-top: 1.875rem;
        }
        
        h2 {
            font-size: 1.5rem;
            margin-top: 1.5rem;
        }
        
        h3 {
            font-size: 1.25rem;
            margin-top: 1.25rem;
        }
        
        .source {
            border: 1px solid #ddd;
            border-radius: 3px;
            padding: 1.5em;
            word-break: break-all;
        }
        
        .callout {
            border-radius: 3px;
            padding: 1rem;
        }
        
        figure {
            margin: 1.25em 0;
            page-break-inside: avoid;
        }
        
        figcaption {
            opacity: 0.5;
            font-size: 85%;
            margin-top: 0.5em;
        }
        
        mark {
            background-color: transparent;
        }
        
        .indented {
            padding-left: 1.5em;
        }
        
        hr {
            background: transparent;
            display: block;
            width: 100%;
            height: 1px;
            visibility: visible;
            border: none;
            border-bottom: 1px solid rgba(55, 53, 47, 0.09);
        }
        
        img {
            max-width: 100%;
        }
        
        @media only print {
            img {
                max-height: 100vh;
                object-fit: contain;
            }
        }
        
        @page {
            margin: 1in;
        }
        
        .collection-content {
            font-size: 0.875rem;
        }
        
        .column-list {
            display: flex;
            justify-content: space-between;
        }
        
        .column {
            padding: 0 1em;
        }
        
        .column:first-child {
            padding-left: 0;
        }
        
        .column:last-child {
            padding-right: 0;
        }
        
        .table_of_contents-item {
            display: block;
            font-size: 0.875rem;
            line-height: 1.3;
            padding: 0.125rem;
        }
        
        .table_of_contents-indent-1 {
            margin-left: 1.5rem;
        }
        
        .table_of_contents-indent-2 {
            margin-left: 3rem;
        }
        
        .table_of_contents-indent-3 {
            margin-left: 4.5rem;
        }
        
        .table_of_contents-link {
            text-decoration: none;
            opacity: 0.7;
            border-bottom: 1px solid rgba(55, 53, 47, 0.18);
        }
        
        table,
        th,
        td {
            border: 1px solid rgba(55, 53, 47, 0.09);
            border-collapse: collapse;
        }
        
        table {
            border-left: none;
            border-right: none;
        }
        
        th,
        td {
            font-weight: normal;
            padding: 0.25em 0.5em;
            line-height: 1.5;
            min-height: 1.5em;
            text-align: left;
        }
        
        th {
            color: rgba(55, 53, 47, 0.6);
        }
        
        ol,
        ul {
            margin: 0;
            margin-block-start: 0.6em;
            margin-block-end: 0.6em;
        }
        
        li > ol:first-child,
        li > ul:first-child {
            margin-block-start: 0.6em;
        }
        
        ul > li {
            list-style: disc;
        }
        
        ul.to-do-list {
            text-indent: -1.7em;
        }
        
        ul.to-do-list > li {
            list-style: none;
        }
        
        .to-do-children-checked {
            text-decoration: line-through;
            opacity: 0.375;
        }
        
        ul.toggle > li {
            list-style: none;
        }
        
        ul {
            padding-inline-start: 1.7em;
        }
        
        ul > li {
            padding-left: 0.1em;
        }
        
        ol {
            padding-inline-start: 1.6em;
        }
        
        ol > li {
            padding-left: 0.2em;
        }
        
        .mono ol {
            padding-inline-start: 2em;
        }
        
        .mono ol > li {
            text-indent: -0.4em;
        }
        
        .toggle {
            padding-inline-start: 0em;
            list-style-type: none;
        }
        
        /* Indent toggle children */
        .toggle > li > details {
            padding-left: 1.7em;
        }
        
        .toggle > li > details > summary {
            margin-left: -1.1em;
        }
        
        .selected-value {
            display: inline-block;
            padding: 0 0.5em;
            background: rgba(206, 205, 202, 0.5);
            border-radius: 3px;
            margin-right: 0.5em;
            margin-top: 0.3em;
            margin-bottom: 0.3em;
            white-space: nowrap;
        }
        
        .collection-title {
            display: inline-block;
            margin-right: 1em;
        }
        
        time {
            opacity: 0.5;
        }
        
        .icon {
            display: inline-block;
            max-width: 1.2em;
            max-height: 1.2em;
            text-decoration: none;
            vertical-align: text-bottom;
            margin-right: 0.5em;
        }
        
        img.icon {
            border-radius: 3px;
        }
        
        .user-icon {
            width: 1.5em;
            height: 1.5em;
            border-radius: 100%;
            margin-right: 0.5rem;
        }
        
        .user-icon-inner {
            font-size: 0.8em;
        }
        
        .text-icon {
            border: 1px solid #000;
            text-align: center;
        }
        
        .page-cover-image {
            display: block;
            object-fit: cover;
            width: 100%;
            height: 30vh;
        }
        
        .page-header-icon {
            font-size: 3rem;
            margin-bottom: 1rem;
        }
        
        .page-header-icon-with-cover {
            margin-top: -0.72em;
            margin-left: 0.07em;
        }
        
        .page-header-icon img {
            border-radius: 3px;
        }
        
        .link-to-page {
            margin: 1em 0;
            padding: 0;
            border: none;
            font-weight: 500;
        }
        
        p > .user {
            opacity: 0.5;
        }
        
        td > .user,
        td > time {
            white-space: nowrap;
        }
        
        input[type="checkbox"] {
            transform: scale(1.5);
            margin-right: 0.6em;
            vertical-align: middle;
        }
        
        p {
            margin-top: 0.5em;
            margin-bottom: 0.5em;
        }
        
        .image {
            border: none;
            margin: 1.5em 0;
            padding: 0;
            border-radius: 0;
            text-align: center;
        }
        
        .code,
        code {
            background: rgba(135, 131, 120, 0.15);
            border-radius: 3px;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-size: 85%;
            tab-size: 2;
        }
        
        code {
            color: #eb5757;
        }
        
        .code {
            padding: 1.5em 1em;
        }
        
        .code > code {
            background: none;
            padding: 0;
            font-size: 100%;
            color: inherit;
        }
        
        blockquote {
            font-size: 1.25em;
            margin: 1em 0;
            padding-left: 1em;
            border-left: 3px solid rgb(55, 53, 47);
        }
        
        .bookmark-href {
            font-size: 0.75em;
            opacity: 0.5;
        }
        
        .sans { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
        .code { font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace; }
        .serif { font-family: Lyon-Text, Georgia, KaiTi, STKaiTi, '华文楷体', KaiTi_GB2312, '楷体_GB2312', serif; }
        .mono { font-family: Nitti, 'Microsoft YaHei', '微软雅黑', monospace; }
        .pdf .sans { font-family: Inter, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }
        
        .pdf .code { font-family: Source Code Pro, 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }
        
        .pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, KaiTi, STKaiTi, '华文楷体', KaiTi_GB2312, '楷体_GB2312', serif, 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }
        
        .pdf .mono { font-family: PT Mono, Nitti, 'Microsoft YaHei', '微软雅黑', monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }
        
        .highlight-default {
        }
        .highlight-gray {
            color: rgb(155,154,151);
        }
        .highlight-brown {
            color: rgb(100,71,58);
        }
        .highlight-orange {
            color: rgb(217,115,13);
        }
        .highlight-yellow {
            color: rgb(223,171,1);
        }
        .highlight-teal {
            color: rgb(15,123,108);
        }
        .highlight-blue {
            color: rgb(11,110,153);
        }
        .highlight-purple {
            color: rgb(105,64,165);
        }
        .highlight-pink {
            color: rgb(173,26,114);
        }
        .highlight-red {
            color: rgb(224,62,62);
        }
        .highlight-gray_background {
            background: rgb(235,236,237);
        }
        .highlight-brown_background {
            background: rgb(233,229,227);
        }
        .highlight-orange_background {
            background: rgb(250,235,221);
        }
        .highlight-yellow_background {
            background: rgb(251,243,219);
        }
        .highlight-teal_background {
            background: rgb(221,237,234);
        }
        .highlight-blue_background {
            background: rgb(221,235,241);
        }
        .highlight-purple_background {
            background: rgb(234,228,242);
        }
        .highlight-pink_background {
            background: rgb(244,223,235);
        }
        .highlight-red_background {
            background: rgb(251,228,228);
        }
        .block-color-default {
            color: inherit;
            fill: inherit;
        }
        .block-color-gray {
            color: rgba(55, 53, 47, 0.6);
            fill: rgba(55, 53, 47, 0.6);
        }
        .block-color-brown {
            color: rgb(100,71,58);
            fill: rgb(100,71,58);
        }
        .block-color-orange {
            color: rgb(217,115,13);
            fill: rgb(217,115,13);
        }
        .block-color-yellow {
            color: rgb(223,171,1);
            fill: rgb(223,171,1);
        }
        .block-color-teal {
            color: rgb(15,123,108);
            fill: rgb(15,123,108);
        }
        .block-color-blue {
            color: rgb(11,110,153);
            fill: rgb(11,110,153);
        }
        .block-color-purple {
            color: rgb(105,64,165);
            fill: rgb(105,64,165);
        }
        .block-color-pink {
            color: rgb(173,26,114);
            fill: rgb(173,26,114);
        }
        .block-color-red {
            color: rgb(224,62,62);
            fill: rgb(224,62,62);
        }
        .block-color-gray_background {
            background: rgb(235,236,237);
        }
        .block-color-brown_background {
            background: rgb(233,229,227);
        }
        .block-color-orange_background {
            background: rgb(250,235,221);
        }
        .block-color-yellow_background {
            background: rgb(251,243,219);
        }
        .block-color-teal_background {
            background: rgb(221,237,234);
        }
        .block-color-blue_background {
            background: rgb(221,235,241);
        }
        .block-color-purple_background {
            background: rgb(234,228,242);
        }
        .block-color-pink_background {
            background: rgb(244,223,235);
        }
        .block-color-red_background {
            background: rgb(251,228,228);
        }
        
        .checkbox {
            display: inline-flex;
            vertical-align: text-bottom;
            width: 16;
            height: 16;
            background-size: 16px;
            margin-left: 2px;
            margin-right: 5px;
        }
        
        .checkbox-on {
            background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
        }
        
        .checkbox-off {
            background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
        }
            
        </style> -->
</head>

<body>

    <div id="wrapper">

        <div class="container">

            <section id="top" class="section docs-heading">

                <div class="row">
                    <div class="col-md-12">
                        <div class="big-title text-center">
                            <h1>CS224n: Results of Group Study</h1>
                            <p class="lead">written by 
                                <a href="https://github.com/emjayahn">EmjayAhn</a>,
                                <a href="https://github.com/jaeseongyou">JaeseongYou</a>,
                                <a href="https://github.com/donariumdebbie">donariumdebbie</a>
                            </p>
                        </div>
                        <!-- end title -->
                    </div>
                    <!-- end 12 -->
                </div>
                <!-- end row -->

                <hr>

            </section>
            <!-- end section -->

            <div class="row">

                <div class="col-md-3">
                    <nav class="docs-sidebar" data-spy="affix" data-offset-top="300" data-offset-bottom="200" role="navigation">
                        <ul class="nav">
                            <li><a href="#line1">Lecture 01</a></li>
                            <li><a href="#line2">Lecture 02</a></li>
                            <li><a href="#line3">Lecture 04</a></li>
                            <li><a href="#line4">Lecture 05</a></li>
                            <li><a href="#line5">Lecture 06</a></li>
                            <li><a href="#line6">Our Questions</a></li>
                            <!-- <li><a href="#line3">How to Install Theme</a></li>
                            <li><a href="#line4">Necessary Plugins</a></li>
                            <li><a href="#line5">Creating Blog Pages</a></li>
                            <li><a href="#line6">Revolution Slider</a></li>
                            <li><a href="#line7">How to Use Option Panel</a>
                                <ul class="nav">
                                    <li><a href="#line7_1">General Options</a></li>
                                    <li><a href="#line7_2">Style Options</a></li>
                                    <li><a href="#line7_3">Header Options</a></li>
                                    <li><a href="#line7_4">Font Options</a></li>
                                    <li><a href="#line7_5">Slider Options</a></li>
                                    <li><a href="#line7_6">Page Options</a></li>
                                    <li><a href="#line7_7">Import & Export</a></li>
                                </ul>
                            </li>
                            <li><a href="#line8">Support Desk</a></li>
                            <li><a href="#line9">Files & Sources</a></li>
                            <li><a href="#line10">Version History (Changelog)</a></li>
                            <li><a href="#line11">Copyright and license</a></li> -->
                        </ul>
                    </nav >
                </div>
                <div class="col-md-9">
                    <section class="welcome">

                        <div class="row">
                            <div class="col-md-12 left-align">
                                <h2 class="dark-text">Introduction<hr></h2>
                                <div class="row">

                                    <div class="col-md-12 full">
                                        <div class="intro1">
                                            <ul>
                                                <li><strong>Lecture Name : </strong>CS224n</li>
                                                <li><strong>Lecture Version : </strong> Winter 2019</li>
                                                <li><strong>Lecture Link  : </strong> <a href="https://www.youtube.com/watch?v=8rXD5-xhemo&t=1s">Standford Univiersity CS224n</a></li>
                                                <li><strong>Page Authors : </strong> <a href="https://github.com/emjayahn">Emjayahn</a>, <a href="https://github.com/jaeseongyou">JaeseongYou</a>,
                                                    <a href="https://github.com/donariumdebbie">donariumdebbie</a></li>
                                            </ul>
                                        </div>

                                        <hr>
                                        <div>
                                            <p> CS224n을 함께 들으면서 공부해봅시다.
                                            </p>
                                            <div class="intro2 clearfix">
                                                <p><i class="fa fa-exclamation-triangle"></i> 저희가 개인적으로 공부한 내용입니다. 아래 내용중 잘못 이해한 내용이 있을 수 있습니다. 
                                                </p>
                                            </div>

                                        </div>
                                    </div>

                                </div>
                                <!-- end row -->
                            </div>
                        </div>
                    </section>

                    <section id="line1" class="section">

                        <div class="row">
                            <div class="col-md-12 left-align">
                                <h2 class="dark-text">Lecture 01<a href="#top">#back to top</a><hr></h2>
                            </div>
                            <!-- end col -->
                        </div>
                        <!-- end row -->

                        <div class="row">
                            <div class="col-md-12">
                                <article id="e7574b2d-8a4d-41f3-8700-8f7077247b8f" class="page sans"><header><h1 class="page-title">written by <a href="https://github.com/emjayahn">EmjayAhn</a></h1></header><div class="page-body"><h1 id="0a6297f1-d0e0-41af-8090-a609232e1fb7" class="">[CS224n] Lecture 1: Introduction and Word Vectors</h1><p id="9756b33e-85e1-4a84-8d14-3626b41f4a92" class="">Standford University 의 CS224n 강의를 듣고 정리하는 글입니다.</p><h2 id="ef927be2-3662-49bd-8850-1c69e86ea48f" class="">1. Human Language and Word Meaning</h2><p id="625ded1b-b2bd-4324-94c4-9bdab090fd87" class=""><strong>Word meaning</strong> 의 뜻 : symbol → idea or thing : Denotational Semantics</p><p id="f34f7656-4bb2-406c-9da7-ea91ae0a1bea" class="">우리가 ’의자’라는 단어를 예로 들자면, 말하는 사람과 듣는 사람 모두 의자와 관련된 특정한 이미지와 아이디어 등을 생각해 볼 수 있다. 이렇게 단어에 대해 관념적인 것을 word meaning 이라고 할 수 있다. 그렇다면 컴퓨터에서 사용할 수 있는 <strong>word meaning</strong> 은 어떤 것이 있을까?</p><h3 id="58aef56d-f8e4-463a-b865-1165af348af0" class="">In Computer Word Representations</h3><ol id="1d0b8fcc-cc91-4bce-abf1-6c31f6fff526" class="numbered-list" start="1"><li>WordNet: synonym 과 hypernyms 의 집합으로 표현 (e.g: nltk wordnet)<ul id="774ad5b8-9256-40b9-a200-7581c8992c3d" class="bulleted-list"><li>아주 귀한 dataset 이지만,</li></ul><ul id="6566d47a-4df6-4550-99aa-8f8cb52aff8d" class="bulleted-list"><li>단점: Nuance (뉘앙스)를 담아내지 못함, 새로운 단어에 대해 사람이 직접 가공하여 추가해주어야 한다. 단어의 동의어에 대해 계산할 수 없음</li></ul></li></ol><ol id="2fe002f4-f51e-4faa-b062-28d3c97e15b3" class="numbered-list" start="2"><li>Discrete Symbols: One-hot-vectors<ul id="c39a9a2b-ff10-4fd3-b9f1-68713325192a" class="bulleted-list"><li>단점 : 벡터의 차원이 우리가 가지고 있는 단어의 갯수만큼 커지게 된다. Corpus 의 구성 단어가 커질 수록 계산 해야하는 벡터 차원이 매우 커진다. 이는 컴퓨팅 자원의 부족으로 인한 현실적 구현의 어려움을 야기시킨다.</li></ul><ul id="3b220304-f27c-4441-80df-42060ddb2487" class="bulleted-list"><li>One-hot-Vector 는 Vector Space 에서 모두 서로 orthogonal (직교)한다. 즉, similarity 가 모두 0으로 단어간의 관계를 알기 힘들다.</li></ul></li></ol><ol id="c9bed0e7-3e87-4dc3-916f-30f3b51f771f" class="numbered-list" start="3"><li>By CONTEXT: Word Vecttors<ul id="8efefc45-1db9-44af-871b-684a33c13b09" class="bulleted-list"><li>“You shall know a word by the company it keeps” (J.R.Firth)</li></ul><ul id="6376f52f-178c-4ea0-ae16-ba504a79b005" class="bulleted-list"><li>어떤 특정 단어 w 가 나온다는 것은, 그 주변 단어들(context)이 있기 때문이다.</li></ul><ul id="cd6166c5-bc0d-4398-8607-9c1b5153fe36" class="bulleted-list"><li>Word Vectors == Word Embeddings == Word Representations == Distributed Representation</li></ul></li></ol><h2 id="9a7079b1-9991-41af-a71d-72d43bdd4587" class="">2. Word2vec Introduction</h2><p id="680775a9-d92d-4d2b-b637-703701faaf66" class="">Word2vec(Mikolov et al. 2013) 은 Word Vectors 를 만들기 위한 알고리즘 중 가장 기초 뼈대를 이루는 알고리즘이다.</p><ol id="104f534e-5c30-4f64-9e84-d6861517d08f" class="numbered-list" start="1"><li>가지고 있는 텍스트 데이터를 구성하는 큰 CORPUS</li></ol><ol id="ad31382f-2b32-4ec2-9a6e-b94ed5fca6cd" class="numbered-list" start="2"><li>Corpus 의 모든 단어는 RandomVector 로 시작한다.</li></ol><ol id="d992ae61-595f-447d-8d8d-119c25f64207" class="numbered-list" start="3"><li>각 position t 에 대해, 중심단어 c(center) 와 주변단어 o(outside) 를 생각할 수 있다.</li></ol><ol id="384c5626-d5d1-4008-baf7-0a970d203224" class="numbered-list" start="4"><li>중심단어 c 가 주어질 때, 주변단어 o 의 확률을 구하기 위해(skip-gram)(반대로도 가능(cbow): 주변단어가 주어질 때, 중심단어의 확률을 구하는 방법), 중심단어 c 와 주변단어 o, word 벡터들에 대해 similarity를 구한다.</li></ol><ol id="9c1f6417-8fc6-4edf-9fde-f3f63c374134" class="numbered-list" start="5"><li>위의 확률을 최대화 하기 위해 word vector 를 updating 한다.</li></ol><figure id="93ba71ce-053a-439a-8a5f-4d8a8381778a" class="image"><a href="Untitled-cc28ca48-12e4-4a20-a6c4-b0be86d339d8.png"><img src="Untitled-cc28ca48-12e4-4a20-a6c4-b0be86d339d8.png"/></a></figure><h2 id="8439b54e-8217-4402-8a1f-3a629541f71e" class="">3. Word2vec objective function gradients</h2><p id="61f2634d-7b5b-4201-ac45-560cb0b2307b" class="">문장의 특정 위치 t 에 대해 (t = 1, …, T), 중심단어w_j가 주어졌을때, 주변단어를 한정하는 window size m 에 대해 주변단어들을 예측하는 Likelihood 는 다음과 같습니다. likelihood 식을 해석해보면, 중심단어 w_t 에 대해 중심단어를 중심으로 window 사이즈 2m 개의 단어들의 확률을 모두 곱하고, T 개의 단어 갯수에 대해 또 다시 모두 곱합니다.</p><p id="79f602ca-e366-4c7f-89cb-9ae1a464fd43" class="">$$Likelihood\quad L(\theta)= \prod_{t=1}^{T}\prod_{-m \leq j \leq m}P(w_{t+j}|w_{t};\theta)$$</p><p id="1ee0ca14-f029-44a0-aed0-5b1c7a4d1597" class="">목적함수는 likelihood 를 바탕으로 minimize와 average, 계산 편의를 위해 log 를 씌웠다는 것외에 likelihood 와 동일하다.</p><p id="82d29776-5a94-41ad-b448-895fe7c3086e" class="">$$objective function \quad J(\theta) = - \frac{1}{T}logL(\theta)=-\frac{1}{T}\sum_{t=1}^{T}\sum_{-m \leq j \leq m, j\neq0}logP(w_{t+j} | w_{t};\theta)$$</p><p id="33bf6501-f26c-4abb-b95e-706bbdbad380" class="">단어가 등장할 확률을 구하는 방법은 Softmax Function 을 활용합니다.</p><figure id="b2d18958-9caf-415b-9688-1dfe5816445b" class="image"><a href="Untitled-99acc04b-91fd-451c-868f-574d995778b3.png"><img src="Untitled-99acc04b-91fd-451c-868f-574d995778b3.png"/></a></figure><h3 id="0b033c98-99ac-4ddb-b0ac-978553511b22" class="">Objective Function 의 derivative</h3><p id="7ec91303-b052-408d-a030-e17499dc28fc" class="">Objective Function 과 단어 확률 (Softmax) 의 미분을 구하는 과정이다.</p><figure id="3a7b9038-90fe-4a30-96f3-6b73636f4157" class="image"><a href="Untitled-d45516b7-8024-457c-bda2-39092a1f6892.png"><img src="Untitled-d45516b7-8024-457c-bda2-39092a1f6892.png"/></a></figure></div></article>
                            </div>
                            <!-- end col -->
                        </div>
                        <!-- end row -->
                    </section>
                    <!-- end section -->

                    <section id="line2" class="section">

                            <div class="row">
                                <div class="col-md-12 left-align">
                                    <h2 class="dark-text">Lecture 02<a href="#top">#back to top</a><hr></h2>
                                </div>
                                <!-- end col -->
                            </div>
                            <!-- end row -->
    
                            <div class="row">
                                <div class="col-md-12">
                                    <article id="2369deaa-eeb6-4af0-9b57-b84532214592" class="page sans"><header><h1 class="page-title">written by <a href="https://github.com/jaeseongyou">JaeseongYou</a></h1></header><div class="page-body"><h3 id="c6f8d81f-a7a9-45b2-a363-7683d19c6c23" class="">Main Idea of Word2Vec</h3><p id="6ee847b2-e888-4b25-8cff-fb22db6bd1fc" class="">각 단어는 주변에 출현하는 단어들로 정의됩니다.</p><p id="a05e63f8-e10f-4a39-820f-fb5a5cf70909" class="">학습과정에서 벡터는 p(주변단어|중심단어)가 높도록 변화됩니다.</p><p id="9d8a2421-60cc-42b0-b3e4-b754f195ab95" class="">비슷한 단어들을 주변에 갖는 단어들은 임베딩 공간에서 서로 가까이 위치하게 됩니다.</p><h3 id="cebaf8ec-0cd3-4e86-85a9-42991ac7c41a" class="">두 행렬: U와 V</h3><p id="92dfc8e9-9984-47aa-8485-5912049a78e0" class="">[이전 질문] dL/du와 dL/dv를 따로 구해서 u와 v를 각각 업데이트하면 사실상 네트워크 둘을 트레이닝 하는 것으로 보이는데, 왜 u와 v로 나누는 것이 더 간단한가요?</p><p id="d7c704c7-61c6-4389-824a-8a7ca83bded5" class="">[대답] 만약 한 행렬만 쓸 경우, 중심단어가 주변단어 후보로 한 번 등장하게 되는데, 2승 텀이 되고 미분 계산이 더 어려워집니다.</p><p id="a07aaa79-fec0-4dfc-91d4-e8cb2ec0a9ee" class="">모든 단어가 ’the’나 ’a’와 같이 빈번한 단어에 대한 높은 점곱을 가지게 됩니다. 해결법 1: PCA 후 첫번째 component를 제거 (빈도를 나타내므로) 해결법 2: Negative Sampling</p><h3 id="9afb3e7d-5fe2-4642-8cda-7b62496ba13f" class="">최적화</h3><p id="a6e3290f-cded-4dc7-afe5-7659618e92d3" class=""><em>θ</em><em>n</em><em>e</em><em>w</em> = <em>θ</em><em>o</em><em>l</em><em>d</em> − <em>α</em>∇<em>θ</em><em>J</em>(<em>θ</em>)</p><p id="ff67bae8-fb1f-4ade-bb6c-d15246301b54" class="">보통 SGD로 최적화</p><pre id="7a1c89bb-61f2-4bbb-8ae2-7cf07a414194" class="code"><code>while True:window = sample_window(corpus)theta_grad = evaluate_gradient(J, window, theta)theta = theta - alpha * theta_grad</code></pre><h3 id="e70acb69-c762-43eb-8b85-8fd2597e2407" class="">Sparsity 문제</h3><p id="8352696d-8fc0-4e8b-8b74-0163baf0c89f" class="">윈도우 크기가 5일 때, 최대 11개의 단어가 등장, 배치 사이즈가 32이면 최대 출현 단어는 352개가 됩니다. dimension_size * batch_size * num_words = 300 * 32 * 11 = 105,600개 값</p><p id="2742626f-6053-4a48-9e52-a4513cb44c7d" class="">반면 업데이트 해야하는 파라미터는 U와 V 각각 dimension_size * vocabulary_size로 2* 300 * 2,000,000, 약 12억개.</p><h3 id="4cc4ef08-354f-4811-9976-f6cca6694086" class="">해결법1</h3><p id="9f80a12f-1146-4c1b-8668-6b734bdd1c4a" class="">U와 V에서 해당하는 단어의 행만 업데이트 합니다.</p><ul id="6c9a88fb-89cd-4707-a02d-1b902ff80a7a" class="bulleted-list"><li>질문 01: 어떻게 특정 행만 골라서 업데이트 할 수 있나요?</li></ul><ul id="e078d63f-8d78-4aca-b9a2-b4160ee4c5c6" class="bulleted-list"><li>Debbie : 등장한 row의 gradient를 계산해서 matrix add, subtraction 으로 gradient update한다는 말인 것 같습니다.</li></ul><ul id="0c3959c9-9c85-48e9-a0b0-f455e2dfae89" class="bulleted-list"><li>질문 02: 윈도우 크기가 5일 때 매 배치에 대해서 V는 1 행 U는 5 행 비대칭적으로 학습되지 않을까요?</li></ul><h3 id="8fb6c77b-9fae-4b63-947e-e0c7a931b7cf" class="">해결법2</h3><p id="a6870eae-30b8-448f-a52b-1137620856ce" class="">단어 벡터에 대해서 해쉬를 보존합니다.</p><ul id="567d2d5c-72fd-4953-81e4-7468e5174ee6" class="bulleted-list"><li>질문 03: 이게 무슨 뜻일까요??</li></ul><ul id="9a601ef5-93b5-4f10-a2b7-7ae0a8670ea8" class="bulleted-list"><li>Debbie : hash embedding이라는 게 있는데, feature를 제한된 숫자로 변환해주는 함수를 사용하는 방법이라고 합니다. python dict가 사용하는 hash와는 다른 개념이고, hash embedding이 여러 종류가 있고 Bloom Embedding등이 NLP에서 사용된다고 합니다.</li></ul><h3 id="c67a3e59-51c3-44f3-ac2c-322f24f2d95d" class="">Negative Sampling (SG)</h3><p id="de442c67-4d45-4e6a-bbb0-694d7079d4bf" class="">문제를 다르게 Softmax: 2백만 단어 → Binary 로지스틱 회귀: 참 쌍 (주변단어-중심단어) vs. 거짓 쌍(무작위단어-중심단어)</p><ul id="8fe4390c-2042-429e-99f4-d2f1564df4c6" class="bulleted-list"><li>질문 04: $J(\theta) = \frac{1}{T}\sum_{t=1}^TJ_t(\theta)$ 라면 t는 무엇을 의미하나요? (p13)</li></ul><p id="3a6b9f2d-4cd5-4b3c-ac63-fcd31d81302e" class="">$J_{neg-sample}(o, v_c, U) = -log((u_o^Tv_c)) - _{i=1}k(log((u_kTv_c)) $</p><p id="33dfa6af-5b8f-457f-8c3b-08057f2a81a9" class=""><em>P</em>(<em>w</em>) = <em>c</em><em>o</em><em>u</em><em>n</em><em>t</em>(<em>w</em>)3/4/<em>Z</em> 의 확률에 따라 k개 (보통 15)의 단어를 주변단어 외 단어에서 골라 거짓 쌍을 만듭니다. 빈번한 단어가 더 많이 뽑히되 3/4승 때문에 그 빈도가 상대적으로 플랫하게 됩니다.</p><h3 id="4b1e495d-ec38-459b-8644-598db24a368a" class="">동시발생 행렬</h3><p id="696f0076-adfc-4a97-a2eb-fb3b5aad850e" class=""><em>X</em><em>i</em><em>j</em>: <em>w</em><em>o</em><em>r</em><em>d</em><em>i</em>와 <em>w</em><em>o</em><em>r</em><em>d</em><em>j</em>가 윈도우 내 동시발생한 경우를 카운트합니다.</p><h3 id="fd921882-dd83-46d5-9805-49f5eeb39a88" class="">SVD</h3><p id="708ff959-9bba-4181-a1f9-9f4e7b5ba5f3" class=""><em>X</em> = <em>U</em><em>Σ</em><em>V</em><em>T</em> (X가 symmetric이므로 EigenDecomposition과 동일) <em>S</em><em>i</em><em>g</em><em>m</em><em>a</em>의 특이값의 수를 조정해 차원 축소를 할 수 있습니다.</p><h3 id="4e7de017-db45-43bc-b342-3d352bd87ce4" class="">Rohde 트릭</h3><p id="8aeeb89b-bac6-4071-9cd1-fa386228eeb0" class="">​ 카운트 스케일링(빈번한 단어 capping)을 합니다. ​ 윈도우 조정 (유사한 단어를 더 많이 카운트 합니다.) ​ 카운트 대신 피어슨 코릴레이션을 사용합니다.</p><ul id="6a770e94-e142-43b9-9bbd-ebdcc0bce9da" class="bulleted-list"><li>질문 05: 윈도우 조정은 Word2Vec에도 사용되었다고 하는데 Negative Sampling의 P(w)를 말한 것일까요?</li></ul><h3 id="5e72c4d1-d1f5-4f89-a4b9-21d6e00d4f5a" class="">Glove</h3><p id="de118be0-48ba-4b44-b6ed-8e962fed3f0e" class="">동시발생 확률의 비율(=벡터간 차이)로 의미를 인코딩할 수 있습니다.</p><p id="6065d404-cec2-43ab-a549-460539602ed0" class="">방향을 암시하는 단어들의 비율은 커지고, 나머지 관련이 없거나 의미가 일정한 단어의 비율은 1에 가까워집니다.</p><p id="4b49e026-6ab1-4463-b738-4dc477e41254" class="">동시발생 확률의 비율이 임베딩 공간에서 선형적이 되도록 하는 것이 목표입니다.</p><h3 id="903e9c04-7774-470b-9dea-6b4fb5bc2dd1" class="">계산</h3><p id="7d79503a-94cd-42ba-a7d5-4828bf79061e" class="">점곱이 동시발생 확률의 로그가 되도록 하고, 로그 동시발생 확률의 비율을 구하면 자연스럽게 벡터간 차가 나옵니다.</p><p id="5e94a37b-a9fb-49ee-bfcc-81ff924e8b06" class=""><em>w</em><em>i</em> ⋅ <em>w</em><em>j</em> = <em>l</em><em>o</em><em>g</em><em>P</em>(<em>i</em>|<em>j</em>)</p><p id="c4aff26b-3541-42f8-b90e-abc8f0f10d9b" class="">$w_x \cdot (w_a - w_b) = \frac{logP(x|a)}{logP(x|b)}$ (예: a는 ‘얼음’, b가 ’증기’이면 주요 x는 고체와 기체)</p><p id="6a016674-146e-4c2b-8317-7f2afbca416e" class="">$J = \sum_{i,j=1}^Vf(X_{ij})(w_i^Tw_j + b_i + b_j -logX_{ij})^2$ (<em>w</em><em>i</em><em>T</em><em>w</em><em>j</em> − <em>l</em><em>o</em><em>g</em><em>X</em><em>i</em><em>j</em>)2단어 i와 j 사이의 점곱이 동시발생 확률의 로그와 비슷해야 합니다. <em>f</em>(<em>X</em><em>i</em><em>j</em>)는 <em>P</em>(<em>w</em>)3/4와 비슷한 역할을 수행하여 빈번한 단어쌍이 모델에 지나치게 큰 영향을 끼치지 못하도록 합니다.</p><h3 id="c7ab6b7d-6844-40e2-8d08-87cd177053bb" class="">다의어</h3><h3 id="b2ab020f-ef3e-4e85-8b95-328d2521fa39" class="">해결법 1</h3><p id="aeb7091d-dc1c-4597-b533-2d1562205c36" class="">클러스터링을 통해 bank를 bank1, bank2, …, bank5로 나눕니다. 의미간 구분이 불분명한 경우가 있습니다.</p><h3 id="01be2fca-1705-4c6c-84ba-d2e75220ac29" class="">해결법 2</h3><p id="d9aea087-f9e6-429f-bda7-6056c29c05f6" class="">$v_{bank} = <em>1v</em>{bank_1} + <em>2v</em>{bank_2} + etc. $ : weighted average (superposition) of sub-meanings <em>α</em>1 = <em>f</em>1/(<em>f</em>1 + <em>f</em>2 + ...)</p><h3 id="b90ed8ac-ef95-47d5-ac6c-dbf2fa6eabf6" class="">Sparsity 결과</h3><p id="a0fe91e1-ad4d-45ae-ae9b-64b522d7036a" class="">스파스 코딩을 통해 의미를 분해할 수 있습니다.</p><h3 id="ccf08c91-758c-4cef-ac39-4d2b8e1ad01a" class="">평가방식</h3><p id="dd94b806-d547-466c-9b16-8a8d1b0df785" class="">코사인 유사도</p><p id="84ad7816-43af-4a41-8302-2416abb27aa7" class="">아날로지 빈칸</p><p id="453c1f4f-3075-4ea6-8e89-e2534c67d5cb" class="">Word Vector Analogies syntactic: superlatives etc. semantic: city-state etc.</p><p id="0d27aa90-07e4-468f-af87-2d6b3a5f4785" class="">WordSim353</p><h3 id="dfb684ac-eb58-4ae9-85e5-6c1878705245" class="">하이퍼 파라미터</h3><p id="1dcfcfd4-cc43-4363-96ad-6fcc7b06c96a" class="">차원수: 300</p><p id="ea0fbe6a-450e-4ea9-bedb-e0b55d35c2d9" class="">윈도우 크기: 5~10</p><p id="ad406646-cec2-4e4b-a2bc-93d2cb1796ac" class="">학습시간: 24 hrs</p><p id="fc1b9215-4cc1-4c24-95d5-8407b9283617" class="">데이터: Wikipedia</p><h3 id="1badd773-e4d3-4676-bba5-ba63bcdb0cc2" class=""></h3><ul id="b89b6d7d-50ce-422e-8c64-a111d8c7ed7c" class="bulleted-list"><li>Debbie 질문 + (위의 질문 1,3에 답변은 https://eda-ai-lab.tistory.com/122?category=706160 참고하였습니다)</li></ul><ul id="60710a69-f4ac-4e95-9c5e-ec58d302311b" class="bulleted-list"><li>p14 수식 맨 오른쪽에 sigmoid안의 마이너스 이유 좀더 구체적으로?</li></ul></div></article>
                                </div>
                                <!-- end col -->
                            </div>
                            <!-- end row -->
                        </section>
                    

                    <section id="line3" class="section">

                        <div class="row">
                            <div class="col-md-12 left-align">
                                <h2 class="dark-text">Lecture 04<a href="#top">#back to top</a><hr></h2>
                            </div>
                            <!-- end col -->
                        </div>
                        <!-- end row -->

                        <div class="row">
                            <div class="col-md-12">
                                <article id="a2b913c9-718c-442a-96c7-9476092113c4" class="page sans"><header><h1 class="page-title">written by <a href="https://github.com/emjayahn">EmjayAhn</a></h1></header><div class="page-body"><p id="cd3da8ee-1374-4aff-9d68-424e362b3e45" class="">기본적인 computation graph 와 backpropagation 에 관한 내용은 스킵하도록 하겠습니다. 새롭고, 핵심적인 내용만 간추린 내용입니다.</p><h2 id="5440f05a-4b49-42ef-bfca-f9cc04e243f9" class="">1. Word Vector를 Retraining?</h2><p id="b1fbae28-4831-4e8b-8826-009c24fd43bd" class="">Quetion: Retraining 에 대한 판단의 시작 예: TV, telly, television 이 pre-trained word vector 에서는 비슷한  공간에 분포되어있다고 가정할 때, training data 에는 TV 와 telly 단어만 존재하고, test data 에 television이 존재 할 때, word vector 는 어떻게 될 것인가?</p><ul id="0017538f-cdb8-44da-a6eb-1f11bbd96953" class="bulleted-list"><li>Answer: Training data 에 있는 TV 와 telly 에 해당하는 word vector 는 back prop을 진행하면서, 미세하게 업데이트 되며, 같은 방향으로 이동하게 된다. 반면, television에 해당하는 word vector는 weight parameter 업데이트가 일어나지 않으므로, 처음에는 비슷한 공간에 분포 했으나, TV 와 telly 와 멀어지게 된다.</li></ul><p id="e8ca7e3a-fdc7-434d-b8fe-2ff198333f3e" class=""></p><h3 id="3d1d487c-083b-4874-9a90-9bc34c8a7ffd" class="">1-1. Word Vector의 Retraining 여부</h3><ul id="9257cfb6-4a3e-42f5-be2f-c08b1256232a" class="bulleted-list"><li>Word Vector가 학습 할 때는 매우 큰 데이터셋을 가지고 학습하게 된다. 따라서 매우 다양한 단어들이 Corpus 로 존재한다.</li></ul><ul id="5b45f5e0-9df5-4b23-b5a3-b1789aa61df7" class="bulleted-list"><li>Fine tuning?: 우리가 가지고 있는 training dataset 이 매우 작다면, 위에서 든 예에서 직감할 수 있듯이, pre trained vector 를 fine tuning 하게 되면, training set 에 fitting 되는 효과가 있고, 우리가 의도치 않는 weight 의 업데이트가 되거나 혹은 되지 않을 수 있다.</li></ul><p id="7a80d000-68c4-479f-82a4-b9eb81c490ca" class=""></p><h2 id="7c70a2af-adb5-4a81-b83d-b78f0a78b1c0" class="">2. 효율적인 gradient 계산</h2><ul id="632fb759-9579-4d40-a94b-b36358280962" class="bulleted-list"><li>사실 당연하게 여김에도, 우리가 손으로 계산하는 (upstream network * local gradient) 과정을 아래 식에서도 확인 할 수 있듯이, ds/dh, dh/dz term 은 중복되는 과정이다.</li></ul><figure id="04a379ba-b75c-4b32-a1ca-808a4e103981" class="equation">
                                    <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css');</style>
                                    <div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>d</mi><mi>s</mi></mrow><mrow><mi>d</mi><mi>W</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>d</mi><mi>s</mi></mrow><mrow><mi>d</mi><mi>h</mi></mrow></mfrac><mfrac><mrow><mi>d</mi><mi>h</mi></mrow><mrow><mi>d</mi><mi>z</mi></mrow></mfrac><mfrac><mrow><mi>d</mi><mi>z</mi></mrow><mrow><mi>d</mi><mi>W</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{ds}{dW}=\frac{ds}{dh}\frac{dh}{dz}\frac{dz}{dW}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.05744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="mord mathdefault">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.05744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="mord mathdefault">h</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="mord mathdefault">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="mord mathdefault">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div></figure><figure id="2d23851d-3873-4762-8328-ac676384892c" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css');</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>d</mi><mi>s</mi></mrow><mrow><mi>d</mi><mi>b</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>d</mi><mi>s</mi></mrow><mrow><mi>d</mi><mi>h</mi></mrow></mfrac><mfrac><mrow><mi>d</mi><mi>h</mi></mrow><mrow><mi>d</mi><mi>z</mi></mrow></mfrac><mfrac><mrow><mi>d</mi><mi>z</mi></mrow><mrow><mi>d</mi><mi>b</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{ds}{db}=\frac{ds}{dh}\frac{dh}{dz}\frac{dz}{db}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.05744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="mord mathdefault">b</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="mord mathdefault">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.05744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="mord mathdefault">h</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="mord mathdefault">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="mord mathdefault">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="mord mathdefault">b</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div></figure><ul id="8c168303-aad5-4de5-9546-a744bc20ee17" class="bulleted-list"><li>따라서 효율적인 computation을 구현하기 위해서, back propagation에서 upstream network 를 저장하고, 각 parameter 에 대한 local gradient를 구해, 동시에 곱해주어 back prop 을 진행 할 수 있다.</li></ul><p id="47f7f7b2-e3e6-4402-8a35-7de3b8711dd3" class="">
                                </p><h2 id="2fe00021-2afc-431d-a57e-0d284d5d93db" class="">3. Regularization</h2><p id="f75e992a-70bc-4e65-be5f-d4e5e517d6a6" class="">우리가 parameter 가 많아지면 많아질 수록, training error 와  test error 는 낮아지기 마련이다. 하지만, 어느 수준을 넘어가게 되면, training set 에 대해서는 매우 정확해지는 반면, test data(validate data)에 대해서는 generalization에서 실패한 그래프나 수치들을 확인할 수 있다. 따라서, 우리는 반드시 우리가 최적화 하려고 하는 Loss 에 대해 Regularization 을 해주어야만 한다.</p><figure id="8ae93c94-54db-45a3-ac1b-c2e9d91cc940" class="image"><a href="Lecture 04/Untitled.png"><img style="width:792px" src="Lecture 04/Untitled.png"/></a></figure><p id="a8b3cdf3-4fe2-44c0-86ea-32f356b59d03" class="">다음은 L2 Regularization term 이 추가된 loss function 이다. 지겹도록 바왔음에도, 꼭 식을 보면 해석해야겠기에, Model parameter (weight) theta 가 제곱term 으로 너무 커지는 것을 방지하기 위해 lambda 에 비례하여 penalty term 을 추가한다.</p><figure id="2cbc5bbf-09a3-4ba9-af9e-57b2b9d3a5ae" class="image"><a href="Lecture 04/Untitled 1.png"><img style="width:952px" src="Lecture 04/Untitled 1.png"/></a></figure><h2 id="50a0104a-0de9-4b9d-a26d-9eee3153f756" class="">4. Vectorization</h2><p id="8a87c6f4-2453-479d-94de-e3b25746f92e" class="">우리가 forward/backward propagation 을 진행하면서, 각 data의 계산을 looping 하여 계산한다면, 매우 비효율적인 계산 방식이 된다. 우리는 위대한 Vecor/Matrix Multiplication 방법으로, 즉, 모든 data 와 weight을 행렬로 만들어 forward/backward 계산을 진행해야한다. 또한 이렇게 진행했을 때, 가속화 도구인 GPU 활용의 이점을 사용할 수 있다.</p><p id="32e13547-c304-421d-886b-2c5fb1acbc19" class="">
                                </p></div></article>
                            </div>
                            <!-- end col -->
                        </div>
                        <!-- end row -->
                    </section>
                    
                    <section id="line4" class="section">

                        <div class="row">
                            <div class="col-md-12 left-align">
                                <h2 class="dark-text">Lecture 05<a href="#top">#back to top</a><hr></h2>
                            </div>
                            <!-- end col -->
                        </div>
                        <!-- end row -->

                        <div class="row">
                            <div class="col-md-12">
                                <article id="da5a38a8-d3b0-4b8e-a898-1aef87539f2f" class="page sans"><header><h1 class="page-title"># CS224n L6</h1></header><div class="page-body"><p id="f5850540-9f17-4988-bc31-64b2205f5f7e" class="">Language Models and Recurrent Neural Networks</p><p id="a6a50bb0-21da-4072-9d5c-e603c7675fb4" class="">목차</p><p id="0e614848-4c70-49fb-89dc-057137941af9" class="">1. Language Model</p><p id="a0ae2625-8c30-451d-8fc1-a4f44a23d9b2" class="">2. RNN</p><p id="a0fc5279-4794-42ad-9bb2-f5eb4d51221b" class=""># Language Model</p><p id="f41b8dd0-5409-4ad9-9c5c-e1e4437bd847" class="">## Language Modelling 이란</p><p id="e2db61e7-bac2-4d4d-97ae-afbc726487a4" class="">- 단어들의 나열에서 다음 단어가 뭐가 올지 맞추는 것.</p><p id="356ca663-dd73-4e20-8896-0a8a79811990" class="">- formally : sequence of words가 주어졌을 때 다음 단어로 올 단어들의 확률 분포를 계산하는것</p><p id="15ad053f-fa2f-4669-a10a-a82c96e810b2" class="">- 이 기능을 수행하는 것을 Language Model이라 한다.</p><p id="cd248811-c6ec-4329-82bd-bdd3e1445236" class="">- 조건부 확률들의 곱으로 표현</p><p id="0a9e1dbe-b6b5-4c7a-bb8b-60ebfdc23a6e" class="">![](Untitled-35adf92b-438c-48b1-853b-1c8a62993bfd.png)</p><p id="876b313b-6a4a-4e22-b639-3986954ca4b8" class="">- 사용 예시 : 핸드폰의 타자 자동완성 기능, 검색어 자동 완성기능</p><p id="f1252f8e-3ade-4ec3-a22e-f0afc44b6db1" class="">딥러닝 이전의 language model 학습시키는 방법으로 n-gram language model이 있었다.</p><p id="3cd73b5c-e1c8-4b35-8192-262a772f66a7" class="">## N-gram Language Model이란?</p><p id="d01f57f0-b201-452d-856e-4d2c6e46fb51" class="">정의 : n-gram은 n개 연속된 단어 뭉치이다.</p><p id="dfcb878a-357d-404c-baac-27ddfa15b4fd" class="">아이디어 : 얼마나 n개단어뭉치가 통계적으로 같이 나오는가?</p><p id="ac0a334e-fd50-4ee8-ad01-aa57a9c4bfcd" class="">가정 : t+1번째의 x가 뭐가올지는 n-1개 앞에 있는 단어에 의해서만 결정된다고 하자.</p><p id="5d50af9b-5c34-4d28-8d80-29aefe73672a" class="">![](Untitled-3038ebcd-0050-4784-91b2-6ebb9a6404ae.png)</p><p id="0d7520eb-f0ff-43bd-8bc2-7a828c00cfe7" class="">이렇게 표현할 수 있다. n-gram과 (n-1)gram의 확률은 counting base. 학습 뭉치에 나타난 빈도수로.</p><p id="9e1872b9-aa72-47ea-986c-f926c924b96b" class="">예 ) 우리가 가지고 있는 학습 뭉치에 파이 라고 시작된 단어 뒤에 파이썬 파이콘 파이리 등이 있다면, 파이 가 등장한 총 횟수 분의 파이썬이 등장한 횟수를 세면 된다.</p><p id="b7c3a67d-6ed9-4556-afee-4c09b2dc2068" class="">문장이 길게 있으면, 4-gram language model을 우리가 사용한다면 앞의 모든 부분은 날아가고 우리가 맞추려고 하는 부분 바로 앞의 3단어만 조건으로 들어간다.</p><p id="e7bf0247-bfb2-4e72-8997-bc38183f374b" class="">![](Untitled-ed06598a-eb1b-40cf-ad17-abd05d7732f2.png)</p><p id="d55f7df1-6281-4318-8eb2-fbf27cf18017" class="">문제: 못본 단어 조합이 나오면 확률이 0으로 간주된다.</p><p id="90ee1ff8-f727-44e3-ae4d-db96769f8869" class="">## Sparsity Problems</p><p id="f756ce99-84d5-488d-8b87-e46196624d79" class="">1. 문제 : 본 적 없는 단어의 등장은 0의 확률로 예측하는 문제점 → partial solution : add small delta to the count of 못본단어. smoothing</p><p id="5fbd0487-4fe3-43e1-a22f-2491a3610dbc" class="">2. 문제 : 분모의 확률, 즉 조건부의 조건이 되는 부분이 등장한 적 없는 경우? 아예 뒤에 무슨 단어가 올지 자체를 계산을 못함 → partial solution : backoff. 예를들어 조건되는부분이 3gram이면 데이터에 존재하는 2gram을 사용한다.</p><p id="02b5e71f-c123-465b-b2de-eee02e551954" class="">n을 키우는 것은 sparsity 문제가 점점 커짐. n을 5 이상은 잘 안쓴다.</p><p id="e6659415-7737-4cd3-9b71-f4c7f8a28c3e" class="">## Storage problems</p><p id="d93fdd0b-34ac-494c-88c2-5eb9621e1804" class="">n-gram을 세기 위해 corpus에 있는 모든 n-gram을 저장해야 한다. n을 키우는 것은 model size를 점점 키우는 꼴.</p><p id="f589573b-6dcc-468c-b92a-515e0dfe8adf" class="">## 실전에서의 n-gram</p><p id="4b58fafa-ed10-4e75-bad7-b807d30c4aba" class="">- 생성한 예제</p><p id="b619a873-a496-4581-8137-9f42b0f09462" class="">![](Untitled-3ae2de78-bcc1-4080-8ceb-51317f23c413.png)</p><p id="c8daa8b0-4eb2-4f25-b571-22ec663a62aa" class="">- 문법적으로 말이 되지만 내용적으로 말이 안됨. 계속 앞의 3 단어만 보고 뒤 단어를 예측하기 때문.</p><p id="c14f15ac-be0b-452c-affd-16df967caf31" class=""># Neural Language Model</p><p id="e6248197-0b2a-4d68-86a6-d1597f2ed55f" class="">window based neural model을 만드는 방법</p><p id="7e13ed45-7cf3-4a05-b8b3-46bf2811a451" class="">![](Untitled-8664f154-6966-4e1c-8424-0989b8427147.png)</p><p id="fb3e24ee-3120-42aa-9252-86d5dd24ddfa" class="">## fixed window neural language model</p><p id="6dbed32e-43ba-4284-a00e-80243a6ac8e5" class="">1. 단어들을 one-hot으로 바꾼다</p><p id="c5eda56d-00ab-43ef-bb46-51a949b89ef8" class="">2. word embedding을 한다.</p><p id="8ac522c7-6778-455e-839f-9d52a361c566" class="">3. hidden layer를 통해 nonlinearity</p><p id="63d6d328-516a-4ff2-8eda-1f63db104400" class="">4. 예측하는 단어의 distribution을 출력한다.</p><p id="7a9349f3-25c7-46a6-bf5e-3b884dd18bc3" class="">![](Untitled-755951e2-2f3f-4f3f-8547-d1baeeffc1e8.png)</p><p id="c1dc4473-f692-4b8e-a9b9-638a31d11e3d" class="">## 장점</p><p id="aa8668e8-7bb7-4747-bd1f-8a1562899438" class="">1. sparsity문제가 사라진다. 안 본 chunk도 확률 부여 가능</p><p id="75607b58-bad0-45e7-98f9-9cb96f5b246e" class="">2. n-gram을 전부 다 저장할 필요가 없다. (embedding)</p><p id="fa4021b2-4e3a-40a2-a012-b93b642307be" class="">## 해결안된 문제</p><p id="ee94af2b-a4c5-434d-9f13-0d024c52dca2" class="">1. fixed window가 너무 작다.</p><p id="23e197d4-db6d-404a-a6a1-80fd2cb5fc8d" class="">2. window크기를 키우면 W가 커진다.</p><p id="50d92abc-6c15-44e4-9561-4224bce30480" class="">3. 문장에서 멀리있는 단어를 반영하자고 그렇다고 window를 한도없이 키울수는없다.</p><p id="2e1e2e53-a5ba-4801-9f82-9cf840beab33" class="">4. no symmetry (?)</p><p id="c6a1570b-dd66-4395-89b9-08c2db39aec0" class=""># Recurrent Neural networks (RNN)</p><p id="fe116631-3285-47a1-9a78-d2ec57bc6bcb" class="">- neural architecture의 하나로 다음과 같은 구조를 띠고 있다.</p><p id="a2639108-60e9-4980-a609-2b5f94198521" class="">![](Untitled-bf38484f-57f4-472e-aa7f-eefab7cb8622.png)</p><p id="3c5ce6ce-eacb-4d02-b6c0-cffb204ac9c5" class="">- hidden state갯수 = input 갯수</p><p id="f5b15a2d-f8d2-4978-b061-976cfd94c092" class="">- W_h, W_e 동시학습</p><p id="0b58617c-80f6-49aa-9555-2b44644e758f" class="">- embedding : pretrain사용하거나, finetuning하거나, initialize해서 새로 학습하거나.</p><p id="77842d82-193d-4a32-b38b-6dd30d2b9001" class="">- h0 : 아무거나. parameter / zerovec</p><p id="1f9566c1-324e-4b4b-91a1-f55423369a17" class="">## RNN language model</p><p id="3de40155-907a-4ff2-8368-da124e058fe5" class="">### 장점</p><p id="b92c3938-a2d8-465c-8638-0f876158b711" class="">- input length 아무거나 됨</p><p id="6785a1be-8c92-475a-b406-fba8618395e7" class="">- window size에 구애받지 않고 many steps back의 정보를 가질수 있음</p><p id="f16ab94f-5612-4035-80fe-ce9984ea1061" class="">- input이 커진다고 해서 model size가 커지지 않음</p><p id="ecedd3a9-961c-4491-98da-558ad8885214" class="">- same weight가 계속 쓰임. symmetry가 있음.</p><p id="28ae7a6b-d507-483c-aef2-b4d52fdcd8a5" class="">### 단점</p><p id="5817a96a-07e5-4a86-8aa8-18de193dffea" class="">- 아주느리다</p><p id="75cddd9f-fa79-4c0e-82ef-d6bc477a903f" class="">- 사실은 many steps back 정보 가져오기 어렵다. (뒤에서 더 다룰 것)</p><p id="ca99a8b6-31a8-41da-82e0-c983b35ede01" class="">그 외</p><p id="d4ac1602-7099-4edb-9085-a23d1a64f7b5" class="">- input length가 Wh에 영향을 주지는 않는다 (?)</p><p id="347705cc-9860-445e-98ab-1d0475509794" class="">## RNN LM을 학습시키는 방법</p><p id="1003e795-1d49-4249-8846-ca7ec5a0f88e" class="">- corpus를 단위별로 rnn-lm에 하나씩 넣으며 timestep마다 yt를 계산한다. (모든단어의 확률분포 계산)</p><p id="bc0c5685-2425-48e2-a785-577e5ff891ab" class="">- Loss function</p><p id="ad1c1dee-79cf-442a-a911-153e8aed46fb" class="">-</p><p id="6a937d29-ae3c-48a8-873b-b2aa5d5849f2" class="">![](Untitled-45b75a16-5685-4f65-8016-5a4e49629cef.png)</p><p id="744ec6ee-a0a0-4166-9658-5f01c266f4f1" class="">- overall Loss</p><p id="90a238e3-fca6-416a-9b72-599244361890" class="">![](Untitled-d911e6bc-b620-418e-8a24-44e14505f80a.png)</p><p id="a08a2853-083a-4983-b83d-c65a8739ed14" class="">![](Untitled-00b5e96d-7459-4119-9997-63af0983f92f.png)</p><p id="1ab68ecc-4b81-4f10-9adf-cb3aa67f4814" class="">- 그러나 이렇게 전체 corpus에 대해 loss와 gradient계산하는것은 계산량이 너무 많다.</p><p id="0e35cfb7-1794-4cfb-b6ac-3cae679d5f3c" class="">- x를 sentence단위나 document 단위로 하는 것을 한다.</p><p id="5be6e6c6-3815-46b3-a8f7-a82e398c7093" class="">## Backpropagation for RNN</p><p id="1f34e717-277c-46da-b64b-5b33b4e79639" class="">- derivative of Jt(theta) w.r.t. W_h 를 다음과 같이 계산한다.</p><p id="6064ce8f-cfd0-426c-8d0d-16268c724582" class="">![](Untitled-ce7f67c4-52e1-45c9-a536-859f58def766.png)</p><p id="7a28d6bd-a0fc-41df-85ff-97603389ca4f" class="">- chain rule 사용</p><p id="0918a237-4205-4180-96d1-91774b52d9a6" class="">![](Untitled-e81d4a4c-ca85-489c-958b-574047285e72.png)</p><p id="cbe9e570-3d36-46ef-89f8-db8a35ffa612" class="">![](Untitled-e57b9184-4dd7-4063-b1bd-d5134b286708.png)</p><p id="90d60a16-2ad3-4e14-a38a-8e59b183908f" class="">## Generating text with a RNN LM</p><p id="534e0847-3ce0-4ec3-966e-7d430196a085" class="">- n-gram처럼 rnn lm도 repeated sampling으로 다음단어를 생성한다. (확률분포에서)</p><p id="afb8ec55-e4c3-4b17-8c11-cd334b202267" class="">### 예시</p><p id="089eb941-fe6e-402d-a0f3-13ec3ae32332" class="">- 오바마 스피치</p><p id="d131f782-5f8b-4020-91d1-c5b846dc9cf6" class="">![](Untitled-0ff36f48-5684-49a0-b69d-e13209e33089.png)</p><p id="d9feb928-52c8-4a50-b782-56ae92cf2680" class="">n-gram lm 보다는 훨씬 낫지만 아직도 incoherent하다</p><p id="de9e9ecb-4092-4105-b213-2e603ba56a3a" class="">- 해리포터</p><p id="8723fe7e-8d7e-4592-8800-d1940f806d99" class="">![](Untitled-9a3780ea-e4d5-4b26-b8c1-a294c1f8eb52.png)</p><p id="cbb00d96-e857-4b27-968f-24a008157619" class="">따옴표를 닫는 것도 인상적.</p><p id="635d60a7-9454-47ef-ab6d-8aec654a2b72" class="">- 레시피</p><p id="8ae83f3c-9b01-4726-94ab-6dc65eb33341" class="">![](Untitled-250945f9-0ed7-4fb9-b830-60ddbe4f5886.png)</p><p id="f141928f-824f-4c30-b39d-f7db77515c0a" class="">- paint color names</p><p id="66293811-fc68-4c47-ac30-14565ef3ad56" class="">![](Untitled-7eb0e786-b716-4ad3-b53d-390f5223f94f.png)</p><p id="a582e33a-4b53-4c43-9061-30ca2ef72eca" class="">character level RNN으로 학습시킨 것이다.</p><p id="344efee8-5d3c-46f1-9ee8-cf2c7e180a33" class="">## Evaluating Language Models (평가하기)</p><p id="a3aeb9db-dc91-4e8e-91fe-88df203630c0" class="">- metric : perplexity</p><p id="354c0409-a90e-4c2f-b272-1115c8a5fe95" class="">![](Untitled-560a1f42-519e-40cf-b916-afd10bffc755.png)</p><p id="e63b104c-b98c-4917-a9ea-8d00abff6d24" class="">낮을수록 좋다.</p><p id="e17275c2-3e32-4e6d-b4a1-c0b3f5eff137" class="">![](Untitled-4fb372bc-89bc-4004-95d0-f964ce8838a4.png)</p><p id="a7c5587c-a870-44f2-9a34-715d09895e88" class="">RNN LM이 n-gram LM보다 perplexity가 점점 좋아졌다.</p><p id="3aa9509e-7e68-4c4a-a55e-435bb0d08ff9" class="">## 우리가 Language Modelling을 신경쓰는 이유</p><p id="7fafc820-f2b4-4fa2-90c4-5c12728d426e" class="">- measure our progress on understanding language.</p><p id="4c6c5058-0df9-428b-af85-62db58586f3a" class="">- 적용 분야</p><p id="331fc7e4-676b-4f73-9a69-e81568979688" class="">- predictive typing</p><p id="07033f9c-315f-47b2-b764-82ba44b20115" class="">- speech recognition</p><p id="1fbc5724-eb71-4c94-a561-0c72e91e3da5" class="">- handwriting recognition</p><p id="c89c68ec-229f-45aa-872e-813182963e4b" class="">- spelling/grammar correction</p><p id="f22e2509-c129-4e80-8c8f-805a2c71ef10" class="">- authorship identificiation</p><p id="0c99159f-36e5-4664-a2a0-387e962b0424" class="">- machine translation</p><p id="b319a685-b515-4639-ad4c-87e614be7a2f" class="">- summarization</p><p id="276bfe96-e0f3-4ac5-94fc-67cc86fc0866" class="">- dialogue</p><p id="b7740c37-cc1e-4d91-a20d-390ae2d2872a" class="">- etc</p><p id="40127a30-c4d9-4399-a757-04741b4dca72" class="">### 그 외 RNN을 사용</p><p id="c500d59f-dbf7-4e88-8901-29623c8f0982" class="">- pos tagging</p><p id="c92d4acf-5b7b-4797-bc8c-035626f5dedb" class="">- named entity recognition</p><p id="5b5e71ce-cd60-4b47-bef8-a9e1e6b31671" class="">- sentence classification</p><p id="87beb9ed-6844-4280-bf01-a7d692d6e0ac" class="">- question answering</p></div></article>
                            </div>
                            <!-- end col -->
                        </div>
                        <!-- end row -->
                    </section>

                    <section id="line5" class="section">

                        <div class="row">
                            <div class="col-md-12 left-align">
                                <h2 class="dark-text">Lecture 06<a href="#top">#back to top</a><hr></h2>
                            </div>
                            <!-- end col -->
                        </div>
                        <!-- end row -->

                        <div class="row">
                            <div class="col-md-12">
                                <article id="681adb84-8ac6-4f7f-8e2e-a8ec8600d56f" class="page sans"><header><h1 class="page-title">written by <a href="https://github.com/donariumdebbie">donariumdebbie</a></h1></header><div class="page-body"><h2 id="3606f0a2-255f-46fa-a5c6-3ad8c48e3ba9" class="">두가지 언어학적 구조</h2><p id="8b02a896-df75-4501-a48d-113664d653ac" class="">​ (1) 구-구조 문법</p><p id="2e356973-e2d0-4904-a286-721b7b7d7676" class="">​ (2) 의존 문법</p><h2 id="0ff2cda0-225e-491f-9db0-cc3990f4aa06" class="">구-구조문법: 언어학 분야에서의 주요 접근법</h2><p id="3d993b3d-c7cf-4455-a6e1-66b02673520e" class="">NP (명사구): det + (adj) + noun, PP (전치사구): prep + NP 등</p><p id="6b948a47-442a-46d8-b294-12ecde91cb51" class="">단어를 기본유닛으로 삼는 여러 종류의 구를 재귀적으로 쌓은 구조로 문장을 해석</p><h2 id="f485ba9b-72ea-409d-851d-d9027aa08fae" class="">의존문법: 전산 언어학 분야에서의 주요 접근법</h2><p id="cdd7346d-4345-49ec-ba64-8e1cbae85155" class="">A -&gt; B (B가 A에 의존한다)</p><p id="844269dc-1bd8-417d-bc16-cc00e78c3c85" class="">단어를 기본유닛으로 삼아 서로간 의존성을 가장 저수준까지 탐색</p><h2 id="474c3f72-6f90-4be8-a885-565dd843e559" class="">의존문법의 불확실성</h2><p id="98580029-6547-48f1-a51f-7e859471759e" class="">자연어의 구조는 의미/문맥에 의존하여 경제적이지만 불확실성 발생</p><p id="58a0ba3c-be77-4c2e-94a8-f0b797872f80" class="">​ * 의미를 이해하려면 구조를 알아야하지만 구조를 찾으려면 의미를 이해해야 합니다.</p><p id="e403c7b3-d7ae-4a06-b19d-98afdec5c153" class="">종류</p><p id="d6dd9ce8-2676-47d6-824f-7a5ece56a6bf" class="">​ 전치사구 연결 불확실성</p><p id="8bc6c976-04ff-48bf-a2e4-b9ebcd62158a" class="">​ 호응범위 불확실성</p><p id="36cf0a60-18e4-4080-b3bd-7cc8b8aa9817" class="">​ 형용사적 수식언 불확실성</p><p id="19f41848-444b-4024-b071-99d7339da2ba" class="">​ 동사구 연결 불확실성</p><h2 id="91380226-88d2-496a-9e98-7ec41d398155" class="">의존성 파싱 트리뱅크</h2><p id="0114757a-87a6-453a-a95c-ed405ca0deba" class="">디자인 특성</p><p id="68d1705c-3c3b-432d-a82f-2b9421a05906" class="">​ ROOT를 더해서 최상부 노드(보통 동사부)가 의존하는 유일한 노드로 작용토록 합니다.</p><p id="858a2992-2329-4c92-8497-b7f4a81bc4ea" class="">​ acyclic graph</p><p id="27e6edd0-e597-4491-b8f0-716a665b9185" class="">데이터</p><p id="118ae604-7c8a-444d-b83f-c11c31913da0" class="">​ Universal Dependencies: http://universaldependencies.org/</p><p id="09b95f24-0735-4ef3-9ba2-0ee8112e782f" class="">장점</p><p id="a80fe055-9c4b-4443-b86a-7ccaa177ba47" class="">​ 룰 기반 표현법에 비해서 재사용이 가능</p><p id="2dd202b2-84c2-4f34-8db6-ead389d5214a" class="">​ 불확실한 구조에 대해서 정/부정 평가가 가능</p><p id="2aab0c80-5a63-430f-913d-34cc66e6770d" class="">​ 범용성</p><p id="b1f2f70b-73d8-4ed8-8d1d-29fce188d5cf" class="">​ 구조의 빈도 계산 가능</p><h2 id="58099b53-6406-4826-8adc-a93159404d66" class="">파싱에 사용하는 정보 (Dependency Conditioning Preferences)</h2><p id="63128324-bc3e-4455-8a12-1fb0dc6cf49d" class="">1 두 단어 사이 친밀도</p><p id="eb6294da-f336-4ef2-91a1-e57f7bddfefd" class="">2 의존성 거리</p><p id="da4dad2c-36c8-4bef-939c-9c02ed47794d" class="">​3 사이에 오는 단어 (의존성은 보통 동사나 마침표를 가로지르지 않습니다.)</p><p id="bdcd8efe-3a46-436f-aef7-00975a07abc7" class="">​4 헤드의 결합가 (헤드의 양 쪽에 보통 몇 개의 의존부가 나타나는지)</p><p id="d3a0889f-ddcb-44bf-b657-edfbd8cfe5b4" class="">해당 문장에 대한 의존성 파싱을 하지 않은 상태에서 두 단어 사이 친밀도, 의존성 거리, 헤드의 결합가를 어떻게 계산할까 궁금했는데, 아마도 수많은 이진 조건으로 이루어진 피처가 이러한 정보를 표현하는 듯 싶습니다.</p><p id="4feec83d-5780-4edb-b58a-bee8c8c4d950" class=""><strong>&gt;질문</strong>: 의존 화살표가 크로스하는 경우와 하지 않는 경우 트리구조 상에서 변화가 발생하지는 않습니다. 예를 들면 I will give a talk on bootstrapping tomorrow와 I will give a talk tomorrow on bootstrapping은 트리구조상에서는 완벽히 동일합니다. 그렇다면 왜 이런 크로싱을 이슈로 여길까요?</p><h2 id="36e1b684-fbee-4527-8b73-31009fe133fe" class="">Transition-Based Parsing</h2><p id="effe5c64-738f-4fd1-8f3c-51389f8ae28e" class="">1 버퍼에서 스택으로 Shift 액션을 통해서 단어를 넘기면서,</p><p id="6d960561-378a-4bb9-a8cf-84675cc3f2e1" class="">2 스택 내 루트를 제외한 단어 쌍이 발생할 경우,</p><p id="eb2db0f6-6c0a-43e5-88ff-567377dc5ff2" class="">단어 쌍에 대해 LeftArc Reduction 이나 RightArc Reduction 중 하나를 수행합니다.</p><p id="8e5c4176-e731-4d80-a7c4-3c19a83f1e05" class="">다시 말해, 의존성과 라벨을 의존성 뱅크에 넣으면서 자식 부분을 스택에서 제외합니다.</p><p id="62ee3b3e-2dc0-4ca8-bdd4-df5a727dcd96" class="">3 루트와 마지막 단어가 남는 경우,</p><p id="3c3cf638-a7c4-406b-9261-d42e70144acb" class="">루트와 마지막 단어와의 의존성과 라벨을 의존성 뱅크에 넣고 스택을 비우는 동시에 의존성 뱅크를 완성합니다.</p><p id="1e037df8-754d-44e2-99b7-9b3894573664" class="">모든 가능한 선택을 탐색하거나 다이나믹 프로그래밍을 사용해서 보다 효과적으로 탐색할 수 있습니다.</p><p id="c199222f-bced-4290-a108-eda70c29b5f8" class=""><strong>&gt;질문</strong>: 이 모델은 &#x27;학습&#x27;이라는 개념이 있나요? 예를 들면 정답이 없는 unseen 데이터를 받아서 모든 가능한 선택을 늘어놓은 후에는 (정답이 주어지지 않기에) accuracy 계산을 할 수 없는데 가능한 트리 중 가장 좋은 트리를 어떻게 고를 수 있나요?</p><h2 id="96175300-b38e-4aa8-b8b9-a292702dca18" class="">MaltParser</h2><p id="2bdc7a0c-b10f-4b86-9d78-5f5bf2d0b5d4" class="">인풋</p><p id="26c0997e-3c1b-4adc-a63f-26af5e32dab9" class="">​ 수백만 이진 조건 특성</p><p id="7c22f057-a4e5-44b5-8bf2-a417e300dc83" class="">모델</p><p id="412ad92e-7eff-41fd-9597-8f8241bdd984" class="">​ 3가지 (혹은 라벨*2+1) 액션 중 어느 작업을 수행할 지 SVM 등의 분류기를 학습</p><p id="ce7b7c31-f42d-43a1-8c01-e61a2a49db47" class="">결과</p><p id="0846146e-a10b-4896-826e-58a6022fbc67" class="">​ 작업 시퀀스에 대한 서치를 하지 않아도 상당한 정확도 (빔 서치를 할 경우 정확도 향상)</p><p id="83e6a721-d295-4979-ab80-1feb12c85ef1" class="">단점</p><p id="bac20369-095e-4183-8c84-bdd2f8091753" class="">​ 인풋이 sparse, incomplete, expensive</p><h2 id="ca2fae92-e2cb-4a1c-b5f8-91343d2177cd" class="">평가방법</h2><p id="f736cc39-2891-4106-b991-d7af139c61dd" class="">라벨을 제외하고 의존성만 평가</p><p id="19e48ca9-44f6-4ab5-a7ae-df2acb62d288" class="">​ Acc = # of correct dependencies / # of dependencies</p><p id="7fd8a13f-2568-4427-bee0-75762faa6b72" class="">라벨까지 평가</p><p id="ea874725-eca3-4306-9b7e-026ad2ee64d4" class="">​ Acc = # of correct dependencies-labels/# of dependencies-labels</p><h2 id="6b84a36c-2e69-4c7a-8390-7b6cd0494e58" class="">분산 표현 (Distributed Representations)</h2><p id="8890cc78-b543-4b93-bce3-dfa88b6922b1" class="">인풋</p><p id="a6e45595-0a0a-4f69-b84d-a367c3338c3f" class="">단어임베딩: d-차원 벡터 표현</p><p id="a80b15bf-1b3d-4787-83d9-e95359eeb530" class="">POS 임베딩: d-차원 벡터 표현</p><p id="cf282e1f-d390-49b2-a2f0-b31985da7242" class="">의존성 라벨: d-차원 벡터 표현</p><p id="c8eeeed4-fb2e-4f74-a2ec-0943dcc2b8c6" class="">토큰 추출 (모든 토큰을 단어, POS, 의존성 라벨을 concat한 3d-벡터로 표현)</p><p id="85e0b54a-b1c8-45aa-87bd-03c4acd8685e" class="">스택단어 1</p><p id="3d90183e-ce68-40a5-8edd-71f0caedfca6" class="">스택단어 2</p><p id="951441c5-eaa1-4292-a8b7-8e38e0f14b0d" class="">버퍼단어 1</p><p id="1fba726f-8e3f-4653-8bbc-9f03fc9a117c" class="">스택단어 1의 왼쪽 의존성</p><p id="9ccef46a-bd1b-4a35-a738-47cc5957b54d" class="">스택단어 1의 오른쪽 의존성</p><p id="c472b478-e103-4bd9-acc9-450381e7c74b" class="">스택단어 2의 왼쪽 의존성</p><p id="18e8c3fc-1ca4-40d1-80fd-96d4b756a567" class="">스택단어 2의 오른쪽 의존성</p><h2 id="6385d563-4fef-4dea-9204-791fd3913187" class="">모델 구조</h2><p id="53e0b346-33f6-4c7a-b43d-b3a5396e21cf" class="">Input layer: x</p><p id="c688c93b-5b47-4fda-8736-27f8e2fbb268" class="">3d-벡터</p><p id="6b08e73e-81cc-41a6-96cc-7850380d6307" class="">Hidden layer: h</p><p id="a29d29a2-8a27-4893-80d5-5bbadc12ced5" class="">h = ReLU(Wx+b1)</p><p id="d99662da-076c-4fea-aab3-d024cd1090f9" class="">Output layer: y</p><p id="ae4bba5b-fbaf-4248-8fa9-d58ce44d0490" class="">y = softmax(Uh+b2)</p><p id="abe4b568-231b-49e6-951a-1fc320a7cfa1" class="">3차원 확률분포 혹은 (2*|라벨|+1)차원 확률분포</p></div></article>
                            </div>
                            <!-- end col -->
                        </div>
                        <!-- end row -->
                    </section>

                    
                    <section id="line6" class="section">

                            <div class="row">
                                <div class="col-md-12 left-align">
                                    <h2 class="dark-text">Our Questions<a href="#top">#back to top</a><hr></h2>
                                </div>
                                <!-- end col -->
                            </div>
                            <!-- end row -->
    
                            <div class="row">
                                <div class="col-md-12">
                                        <article id="7470ae1e-7112-44db-b8e6-3e13d7bfcab7" class="page sans"><header><h1 class="page-title">EmjayAhn, JaeseongYou, donariumdebbie</h1></header><div class="page-body"><p id="8f980f67-01de-459a-8904-e3947fce2149" class="">
                                            </p><p id="1643f973-7c06-4105-9458-f6ecb0bedeca" class=""><strong>Lecture 4</strong></p><p id="2f6ff958-17c7-40a9-9c87-dd5fc3957318" class="">p42 결국 Theano와 TF 간 차이는 전자는 Symbolic 미분 후자는 Automatic 미분이라는 것인가요?</p><p id="596aaec1-4097-41ab-ab48-7c26bce4973d" class="">p47 gradient checking이라는 것은 딥러닝 워크플로우의 어느 부분에서 사용하나요?</p><p id="6db5eb86-0d85-4cb0-88d0-b50865588c5a" class="">p47 왜 two-sided 근사가 더 효과적인가요?</p><p id="f6f0a190-320d-499d-82a6-3f37816957db" class="">p50 왜 l2 정규화의 람다부분을 상수로 설정하나요? (직관적으로는 파라미터 갯수 k나 정규화 없는 손실함수 출력의 크기에 의존해야 할 것 같은데)</p><p id="18d6043d-bc56-438d-b77c-2e07daa66e36" class="">P55 Xavier 초기화는 레이어의 입출력 노드가 늘어나면 정규분포의 분산이 줄어드는 방식인데, 이 작업이 어째서 다음 레이어의 값을 정규분포로 다양하게 표현하는데 효과적인가요?</p><p id="ef06af8c-bf17-4616-9136-542c32b2bdb1" class="">p57 각자 learning rate 을 찾는 노하우가 있나요? leargning rate 외에도 다양한 최적 parameter 를 찾는 노하우 공유 했으면 좋겠습니다.</p><p id="ed9085fc-7934-45ca-8a72-ec2f377e4bcc" class=""><strong>Lecture 6</strong></p><p id="56de14c4-7163-433e-9ea2-3d573b279318" class="">P21: No symmetry는 무슨 뜻일까요?</p><p id="0f0043e8-deb3-4570-893f-496502e2f7b7" class="">P21: 다른 웨이트에 곱해지더라도 백프롭을 통해서 정보가 공유되지는 않을까요?</p><p id="0c65c3ec-3514-4c69-abb6-78a710058d0b" class="">질문: Training a RNN Language Model: 뒤로 갈수록 (T에 가까울수록) 더 정확해지는 것이 아닌지? 그러면 이 불균등을 해결하기 위해 무언가 웨이팅을 주어야 하지는 않을지?</p></div></article>
                                </div>
                                <!-- end col -->
                            </div>
                            <!-- end row -->
                        </section>

                </div>
                <!-- // end .col -->

            </div>
            <!-- // end .row -->

        </div>
        <!-- // end container -->

    </div>
    <!-- end wrapper -->

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/retina.js"></script>
    <script src="js/jquery.fitvids.js"></script>
    <script src="js/wow.js"></script>
    <script src="js/jquery.prettyPhoto.js"></script>

    <!-- CUSTOM PLUGINS -->
    <script src="js/custom.js"></script>
    <script src="js/main.js"></script>

    <script src="js/syntax-highlighter/scripts/shCore.js"></script>
    <script src="js/syntax-highlighter/scripts/shBrushXml.js"></script>
    <script src="js/syntax-highlighter/scripts/shBrushCss.js"></script>
    <script src="js/syntax-highlighter/scripts/shBrushJScript.js"></script>

</body>

</html>
