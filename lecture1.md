# Lecture 1 정리

## Summary




## Questions

#### Emjay



#### Debbie





#### Jaeseong

- u_o - exepctation(u_o)가 v_c에 대한 손실함수의 편미분이라면 v_c - expectation(v_c)가 u_o에 대한 손실함수의 편미분이 되나요?
- dL/du와 dL/dv를 따로 구해서 u와  v를 각각 업데이트하면 사실상 네트워크 둘을 트레이닝 하는 것으로 보이는데, 왜 u와 v로 나누는 것이 더 간단한가요?
- 트레이닝이 끝난 후 각 단어를 나타내는 벡터로 사용하는 것은 u인가요 v인가요? 
  - *u와 v의 평균을 사용합니다 (lecture 2)*
- 윈도우 내 특정 단어의 등장여부를 bigram 비교가 아닌 ngram 비교로도 손실함수를 만들 수 있을까요?
  말하자면, [학교에: 나는], [학교에: 갑니다] 형태의 학습 데이터를 [학교에: 나는, 갑니다] 바꿀 수 있나요? 
  (예: "나는 학교에 갑니다."의 경우 Ground Truth가 "학교에"에 대해 "나는"과 "갑니다"가 둘 다 1의 값을 취합니다.)

## Notes

 
